{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de75e32c",
   "metadata": {},
   "source": [
    "<p style=\"text-align:right\"> \n",
    "Federico Motta\n",
    "<br/> \n",
    "20-01-1998\n",
    "<br/>\n",
    "fm.federicomotta@gmail.com\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c4f044",
   "metadata": {},
   "source": [
    "<style>\n",
    "h1,h2{\n",
    "    color:#D37171\n",
    "}\n",
    "h3,p,li {\n",
    "    \n",
    "    color: #717070;\n",
    "    line-height:150%\n",
    "}\n",
    "\n",
    ".cont {\n",
    "  width:100%;\n",
    "  text-align:center;\n",
    "  display: inline-grid;\n",
    "  justify-content:center;\n",
    "flex-direction: column;\n",
    "}\n",
    "\n",
    ".t_cont {\n",
    "    width:400px\n",
    "}\n",
    "\n",
    ".fun {\n",
    "    color:blue\n",
    "}\n",
    ".str {\n",
    "    color: #AB312A\n",
    "}\n",
    "\n",
    ".fun,.str{\n",
    "    font-weight:400\n",
    "}\n",
    ".appendix{\n",
    "    font-size:12px;\n",
    "}\n",
    "\n",
    ".img {\n",
    "    width:50%;\n",
    "    margin-top:50px;\n",
    "    margin-bottom:50px;\n",
    "    \n",
    "}\n",
    "\n",
    "tt{\n",
    "    font-weight:800\n",
    "}\n",
    "\n",
    "\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d6611",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\"> D3I - Research Engineer </h1>\n",
    "\n",
    "<h2> Introduction </h2>\n",
    "<h3> Research Background </h3>\n",
    "<p>\n",
    "Scraped social media data allow computational social scientists to understand how users engage with big media events such as social movements and general trends (e.g., Hong & Kim, 2016; Ince et al., 2017; Gleason, 2013). However, if we want to investigate how the event changes and shapes the user behavior across environments, it is possible that analyzing only public data could not be enough. What we usually show publicly on social media reflects only some shards of what our “true self” is (Coi et al., 2020). For instance, we can see that a user supports a social movement hashtag (e.g  <span style=\"color:#1DA1F2\">#BlackLivesMatter</span>) but we don’t know why they support it and in which other ways apart from twitting about it. It is rather hard to interpret beliefs and motivations through short tweets or very limited social interactions in the comment sections. Even if it's hard to find deep personal information about users' beliefs, the Internet is full of places where users can develop and build social interaction in a more intimate and genuine way, which leads eventually to the expression of self-disclosure, social support across users, and a more intimate component of their selves (Lomanowska & Guitton, 2016). One of them could be found in instant messaging environments. For their long-term development and the peer-to-peer structure, we suppose that analyzing private conversations such as the one in Instagram Direct Messages could lead to an enrichment of the general knowledge about users' engagement to media events.\n",
    "</p>\n",
    "\n",
    "<h3> Research Question </h3>\n",
    "<h4> The research question investigates how social media users engage in big-media events inside private environments, such as Instagram Direct Messages. </h4>\n",
    "<p> In particular, we can divide the research question in two more practical: </p>\n",
    "    <ul>\n",
    "    <li> <b>RQ1:</b> can private conversations give quantitative support if treated as anonymized scraping data? In other words, can we predict users engagement to big-media events merging several users' conversations? </li>\n",
    "    <li> <b>RQ2:</b> can private conversations give quantitative support by analyzing several users' conversations, without merging them? That is, analysing every user's conversations separately, and eventually performing Analysis of Variance (ANOVA) through them.</li>\n",
    "    </ul>\n",
    "\n",
    "\n",
    "<h3> Inventing a scenario: the <span style=\"color:#1DA1F2\">#fuckarticles</span>  movement</h3>\n",
    "<p>The internet has done it again. During the day of November 15, 2020 a vast amount of prominent personalities  began posting tweets insinuating that \"grammatical articles\" were completely useless and even \"lame.\"(Figure 1). It is still unclear what started this media phenomenom, but a lot of Twitter users started posting <span style=\"color:#1DA1F2\"> #fuckarticles</span> ungrammatical tweets. Several computational social scientists are now trying to understand the weight and the impact of the phenomenom across different social media platforms. </p> \n",
    "\n",
    "<figure class=\"img\">\n",
    "    <img class=\"image\" src=\"https://i.imgur.com/Usz88m1.png\"> \n",
    "    <figcaption><p><b>Figure 1.</b> <i> On the left: influencers from various worlds twitting on how much they hate articles; on the right: the famous show \"The Office\", which changed the name on the same day.</i></p> </figcaption>\n",
    "    </figure>\n",
    "\n",
    "\n",
    "<h3> Providing an answer to the research question in the fake scenario</h3>\n",
    "<p> To find a research answer to the <span style=\"color:#1DA1F2\"> #fuckarticles</span> movement, we need to:</p>\n",
    "    <ol>\n",
    "        <li> access users' instagram private messages and assess their use of articles </li>\n",
    "        <li> collect all the messages containing grammatical articles before the 15/10 (period1) </li>\n",
    "        <li> collect all the messages containing grammatical articles after the 15/10 (period2) </li>\n",
    "        <li> confront period1 and period2 and see if there are significant differences. </li>\n",
    "     </ol>\n",
    "<p> We will use the Instagram data provided by the D3I infrastracture. In particular, the DDP <tt class=\"str\">'Instagram_data_zenodo'</tt>. </p>\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c48ba",
   "metadata": {},
   "source": [
    "<h3> Creating a script to provide an answer to the research question </h3>\n",
    "<p> The instagram data inside the DDP are standard instagram data that can be downloaded by any user at <a>https://www.instagram.com/accounts/privacy_and_security/</a> in a couple of days. In every directory, we have different json files which contains different information about the user. To find the answer to the research question, we are intereseted in just two of them, namely <tt> messages.json</tt> (where all the messages are stored) and <tt> profile.json</tt> (exclusively for RQ2, where the basic user data such as the username are stored).\n",
    "We will now develop a script that will guide us through the analysis of these 2 files.\n",
    "<br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d630db1",
   "metadata": {},
   "source": [
    "<h2> The script </h2>\n",
    "<p> Given any <tt> messages.json </tt>, <tt>profile.json</tt> from Instagram. The script creates a scalable strcucture able to:</p>\n",
    "<ol>\n",
    "    <li> <a href =\"#read\">Read the json structure</a></li>\n",
    "    <li> <a href =\"#clean\">Clean and anonymize the data </a> </li>\n",
    "    <li> <a href =\"#count\">Find word matches in a given period of time </a> </li>\n",
    "    <li> <a href =\"#find\">Find the json owner (for RQ2) </a>\n",
    "    <li> <a href =\"#both\"> Summarise the process for both RQ1 AND RQ2</a> </li>\n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96430aa7",
   "metadata": {},
   "source": [
    "<h3 id=\"read\">Reading the json structure</h3>\n",
    "<p>We start with understanding the json structure of a single <tt>message.json</tt> in the DDP. In order to do it properly, we create a function, <tt class=\"fun\">find_keys()</tt>, which allows us to see the data type (<tt>dict, list</tt>) and all its unique keys. Let's upload a message.json from a random user: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "212e1c55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'list'>, ['participants', 'conversation'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"materials/100billionfaces_20201021/messages.json\",'r') as jfile:\n",
    "    messages = json.load(jfile) ##loads the json in python\n",
    "\n",
    "def find_keys(jload):\n",
    "    if len(jload) == 1: ## when the json is just a single element, the unique keys are the ones of that single element.\n",
    "        return type(jload), jload.keys()\n",
    "    \n",
    "    keys = []\n",
    "    for m in jload:\n",
    "        try: m.keys()\n",
    "        except AttributeError: return None ## when there are no keys left\n",
    "        else: [keys.append(k) for k in m.keys() if not k in keys] ## appends in keys's list only the unique keys (no duplicates)\n",
    "    return type(jload),keys\n",
    "\n",
    "fk = find_keys(messages)\n",
    "\n",
    "print(fk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0506317",
   "metadata": {},
   "source": [
    "<p> We now know that our json is a list of dict, and every element looks like a chat, as it probably has both <tt class=\"str\">'participants'</tt> and <tt class=\"str\">'conversation'</tt> keys. For the rerearch question we are interested in finding where the corpus, the sender, and the date of the message is located. So I run again <tt class=\"fun\"> find_keys()</tt>  </i> in order to get the <tt class=\"str\">'conversation'</tt> keys (most likely the text is there) in a random element of the list (aka a single chat): </p> \n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01f32bb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'list'>, ['sender', 'created_at', 'story_share', 'text', 'link', 'likes', 'media'])\n"
     ]
    }
   ],
   "source": [
    "random_chat = messages[0]['conversation']\n",
    "fk2 = find_keys(random_chat)\n",
    "\n",
    "print(fk2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dbec2f",
   "metadata": {},
   "source": [
    "<p> it seems that we have found all the elements we need for our research answer! namely: the corpus of the message (<tt class=\"str\">'text'</tt>), the date of the message(<tt class=\"str\">'created_at'</tt>) and the sender of the message (<tt class=\"str\">'sender'</tt>).\n",
    "<br/>\n",
    "    Let's see if they actually contain the information we need. Let's print all of them from a random message from the random chat: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4ca0935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ilike\n",
      "You can check it on https://www.lovedance234.com\n",
      "2020-10-20T08:03:58.405275+00:00\n"
     ]
    }
   ],
   "source": [
    "random_message = random_chat[1]\n",
    "print(random_message[\"sender\"][0:5]) ##respect user's privacy :D\n",
    "print(random_message[\"text\"])\n",
    "print(random_message[\"created_at\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee2e15",
   "metadata": {},
   "source": [
    "<h3 id=\"clean\"> Clean and anonymize the data </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1591351a",
   "metadata": {},
   "source": [
    "<p> Now that we know where to find the date, corpus, and senders, it is time to filter the corpus we want for this particular study in the whole JSON. As we are treating personal data, we also need to anonymize any sensitive information that could be present, such as emails, phone numbers, senders' and sender friends' usernames. </p>\n",
    "<p> let's proceed by defining a function, <tt class=\"fun\"> clean_data()</tt> that will:\n",
    "    <ol>\n",
    "  <li>Select every message which contains proper corpus</li>\n",
    "  <li>Filter only messages written in a given language (it'll be English) </li>\n",
    "  <li>Censure every sensible information </li>\n",
    "  <li>Anonymize the sender of any message</li>\n",
    "  <li>Select only messages sent by the json's owner ( optional, for RQ2)\n",
    "</ol>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d829692b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pycld2 as cld2 ## fast language detector \n",
    "import uuid ## A universally unique identifier, in order to anonymise user Id\n",
    "import re  ## regular expression matching operations similar to those found in Perl \n",
    "\n",
    "def clean_data(data,      ##the messages.json \n",
    "               language,  ## the language we want to filter\n",
    "               sender = None,  ##the sender we want to consider, it's None for RQ1 and the json's owner for RQ2\n",
    "               divide_users = False, ## for RQ1 we don't divide users (False) while for RQ2 we do (True) \n",
    "               ):\n",
    "    \n",
    "    def anonymize_corpus(string): \n",
    "        string = string.lower()                                    ##transform all the text to lower case (it helps with matching regex)\n",
    "        \n",
    "        re_email = re.compile(r'[^@\\s]+@[^@\\s]+\\.[a-zA-Z0-9]+$')   ##regular expresssion for emails\n",
    "        re_email2 = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b') ##regular expresssion for emails n.2 (emails are difficult to anonymise :/)\n",
    "        re_pn = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})') ##regular expression for phone numbers\n",
    "        re_tags = re.compile(r'\\B@\\w+') ##regular expresssion for words that starts with \"@\", meaning all ig usernames\n",
    "        \n",
    "        operations = [(re_email,\"**email**\"),\n",
    "                      (re_email2,\"**email**\"),\n",
    "                      (re_pn, \"**num**\"),\n",
    "                      (re_tags,\"@***\")]\n",
    "        \n",
    "        \n",
    "        for regex,new_string in operations: \n",
    "            string = re.sub(regex, new_string, string)\n",
    "        \n",
    "\n",
    "        return string\n",
    "    \n",
    "    cleaned = []\n",
    "    for chat in data:                              ## we start by iterating through every chat present in data;\n",
    "        conversation = chat[\"conversation\"]        ## we iterate through every chat's conversation;\n",
    "        for message in conversation:               ## and we iterate through the messages in that conversation.\n",
    "            if message.get(\"text\"):                 ## We try to find a message with text, \n",
    "                text = message[\"text\"]                                         ## when we find the corpus \n",
    "                message[\"sender\"] = str(uuid.uuid5(uuid.NAMESPACE_URL,         ## we anonymize the sender.\n",
    "                                    message[\"sender\"]))\n",
    "\n",
    "                if not text:                                                ## we ignore when text=None.\n",
    "                    continue\n",
    "                    \n",
    "                isReliable, textBytesFound, details = cld2.detect(text)     ## we then pass the text into a language detector\n",
    "                optional_con = (message[\"sender\"] \n",
    "                                    == sender) if divide_users else True    ## if we want to select only the owner's messages\n",
    "\n",
    "                if not(isReliable and                                       ## and we consider only the text \n",
    "                details[0][1] == language and                               ## which is accurate from the given language\n",
    "                optional_con):\n",
    "                    continue                                 \n",
    "                                                                                \n",
    "                ele = {}                                            ## we create a new dict\n",
    "                ele[\"date\"] = message[\"created_at\"]                 ## with date,\n",
    "                ele[\"text\"] = anonymize_corpus(text)                ## the anonymized corpus\n",
    "                ele[\"sender\"] = message[\"sender\"]                   ## and the anonymized sender.\n",
    "                cleaned.append(ele)                                 ## We then append every valid dict in a list\n",
    "                \n",
    "    return(cleaned)                                                 ## that we'll return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327be1a2",
   "metadata": {},
   "source": [
    "<p> let's try it on some our random messages to see if it works: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d40552ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- you can check it on https://www.lovedance234.com\n",
      "- of course dummie!\n",
      "- ok! alyssa an text me back at 06-**num**2\n",
      "- did anyone of you already see some smurfen or other gnomes @*** @*** @*** @*** ? please send your pics to **email**\n"
     ]
    }
   ],
   "source": [
    "test_cl_data = clean_data(data=messages,\n",
    "                          language=\"en\")[:4]\n",
    "for ele in test_cl_data:\n",
    "    print(\"-\",ele[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7478d996",
   "metadata": {},
   "source": [
    "<h3 id=\"count\">Find word matches in a given period of time</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acca15ff",
   "metadata": {},
   "source": [
    "<p> Finally, it is time to define <tt class=\"fun\">find_occurencies()</tt> which will be able to:\n",
    "<ol>\n",
    "    <li> filter data between a given date span </li>\n",
    "    <li> select all the matching results given a list of strings </li>\n",
    "</ol>\n",
    "<p> We put <tt class=\"fun\">find_occurencies()</tt> inside a <tt>class</tt>, <tt class=\"fun\">FindMatch()</tt>, so that we can then decide to call different methods with different information about our final results, such as: \n",
    "    <ol>\n",
    "    <li> A list of all the filtered data </li>\n",
    "    <li> A list of all the matching data </li>\n",
    "    <li> A list of all the matching data's corpus, date, and senders </li>\n",
    "        <li> A list of all the unique matching data's corpuses </li>\n",
    "        <li> A ratio of matching data on the filtered data </li>\n",
    "        <li> An overview of the result </li>\n",
    "</ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83097b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter ## container that keeps track of how many times equivalent values are added.\n",
    "import dateutil.parser as dparser ## a date/time string parser (because our 'created_at' is type string, not type date)\n",
    "\n",
    "\n",
    "class FindMatch():\n",
    "    def __init__(self, \n",
    "                 cl_data,  ##the data obtained from clean_data()\n",
    "                 wordlist, ## a list of words we want to find in the messages (e.g., articles)\n",
    "                 period,   ## the time span we want to filter, e.g., a list of two datetime.datetime\n",
    "                 sender=None): ##the json owner (RQ2)\n",
    "        \n",
    "        def find_occurencies(cl_data,\n",
    "                             wordlist,\n",
    "                             period):\n",
    "            \n",
    "            flt_list  = []         ##it will contain all the filtered elements from cl_data\n",
    "            matches_all = []       ## it will contain all the matching elements from flt_list\n",
    "            \n",
    "            for message in cl_data:                                        ## we iterate through messages in the cleaned dataset \n",
    "                date = dparser.parse(message[\"date\"],fuzzy=True)           ## we transform date class str into date class datetime.datetime\n",
    "\n",
    "                if period[0] <= date <= period[1]:                         ## if the message's date is in the given date span\n",
    "                    flt_list.append(message)                               ## we append it in our filtered_data list\n",
    "                    \n",
    "                    if not any(w in message[\"text\"] for w in wordlist):\n",
    "                        continue                                       ## if the message contains at least one match with our word list\n",
    "                    matches_all.append(message)                        ## we append it in our matches_all list\n",
    "                    \n",
    "                        \n",
    "\n",
    "            return {\"filtered_data\" : flt_list,\n",
    "                    \"matches_all\": matches_all }\n",
    "        \n",
    "        res = find_occurencies(cl_data, wordlist, period)\n",
    "        matches_all = res[\"matches_all\"]\n",
    "        filtered_data = res[\"filtered_data\"]\n",
    "        \n",
    "        ## it returns a list of all the elements between a given date span\n",
    "        self.filtered_data = filtered_data\n",
    "        \n",
    "        ## it returns a list of matching elements between a given date span\n",
    "        self.matches_all = matches_all\n",
    "        \n",
    "        ## it returns a list of corpus of all the matching elements\n",
    "        matches_corpus = []\n",
    "        [matches_corpus.append(k[\"text\"]) for k in matches_all]\n",
    "        self.matches_corpus = matches_corpus\n",
    "        \n",
    "        ## it returns a list of date of all the matching elements\n",
    "        matches_date = []\n",
    "        [matches_date.append(k[\"date\"]) for k in matches_all]\n",
    "        self.matches_date = matches_date\n",
    "         \n",
    "        ## it returns a list of senders of all the matching elements    \n",
    "        matches_senders = []\n",
    "        [matches_senders.append(k[\"sender\"]) for k in matches_all]\n",
    "        self.matches_senders = matches_senders\n",
    "        \n",
    "        ## it returns the json owner (useful if divide_users=True)\n",
    "        self.sender = sender if sender else \"Multiple senders, please run FindMatch().senders\"\n",
    "\n",
    "        \n",
    "        ##it returns a list of corpus of all the unique corpus value (to check presence of bot messages)\n",
    "        self.matches_corpus_unique = list(Counter(matches_corpus))\n",
    "        \n",
    "        ## it returns the ratio of matching elements on the elements between a given date span\n",
    "        self.ratio =  len(matches_all)/len(filtered_data) if (len(matches_all)+len(filtered_data) != 0) else None\n",
    "        \n",
    "        ## it returns an overview of the data\n",
    "        self.overview = {\"sender\":sender if sender else \"multiple senders\",\n",
    "                         \"filtered_data\": len(filtered_data),\n",
    "                         \"matches\": len(matches_all),\n",
    "                         \"ratio\":self.ratio }\n",
    "        \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e438b73",
   "metadata": {},
   "source": [
    "<h3 id=\"find\">Finding the json owner (for RQ2)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e9489",
   "metadata": {},
   "source": [
    "<p> This last function is just a way of understanding who is the owner of the json. This will help when we will analyse every single user (RQ2) instead of merging all the data together(RQ1). It will also be helpful for privacy reasons (e.g., we may not have the legal permission to look to other users' messages). </p>\n",
    "\n",
    "<p> We therefore define <tt class=\"fun\">find_sender()</tt> which will look into the directory to find the username in another json file (<tt class=\"str\">\"profile.json\"</tt>). In case there are no <tt>profile.json</tt> files, the function will count the most common username across <tt>messages.json</tt> participants.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "629ada4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sender(data,       ##the messages.json file\n",
    "                directory): ##the directory in which the json files are\n",
    "    sender = \"anon\"\n",
    "    try: \n",
    "        open(directory+\"/profile.json\",'r') ##try to find the profile.json file into directory\n",
    "    except FileNotFoundError:               ## in case you don't find profile.json\n",
    "        user_list = []\n",
    "        for chat in messages:                                 ## iterate through chat's participants \n",
    "            user_list = user_list + chat['participants']      \n",
    "            sndr = Counter(user_list).most_common()[0][0]   ## find the most common participant\n",
    "        \n",
    "        \n",
    "    else: \n",
    "        with open(directory+\"/profile.json\",'r') as jfile:    ## if you find profile.json\n",
    "            profile = json.load(jfile) \n",
    "            sndr = profile[\"username\"]                      ## you can find the username here\n",
    "    anonymized_sender = str(uuid.uuid5(uuid.NAMESPACE_URL, sndr))   ##let's anonymized it!\n",
    "    return(anonymized_sender)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb42294",
   "metadata": {},
   "source": [
    "<h3 id=\"both\"> Summarise the process for both RQ1 AND RQ2</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a3094",
   "metadata": {},
   "source": [
    "<p> We have all the elements we need in order to launch a script with our <tt>messages.json</tt> files! Let's define <tt class=\"fun\">start_match()</tt>, a function that will launch all the functions we previously defined. Given a <tt>data</tt> (multiple messages.json), a <tt>wordlist</tt> (e.g., grammatical articles), and a <tt>period</tt> (our date span) the function will be able to return a clean result on merged json data (RQ1, <tt> divide_users = False </tt>).\n",
    "<br/>\n",
    "In addition, as we also want to investigate on single users occurencies (RQ2, <tt> divide_users = True </tt>), we can also provide a given directory in order to find the sender's name. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfd14466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_match(data,                   ##the messages.json\n",
    "                wordlist,               ## a list of words we want to find in the messages (e.g., articles)\n",
    "                period,                 ##the time span we want to filter, e.g., a list of two datetime.datetime\n",
    "                directory = None,         ##the directory in which the json files are\n",
    "                language = \"en\",\n",
    "                divide_users = False,):   ## False for RQ1 and True for RQ2  \n",
    "                \n",
    "    ## if we want to investigate on single users (RQ2)\n",
    "    if divide_users:         \n",
    "        \n",
    "        ##we first need to find our sender \n",
    "        sender = find_sender(data = data, \n",
    "                             directory = directory) \n",
    "        \n",
    "        ##we then clean our data\n",
    "        cl_data = clean_data(data = data,     \n",
    "                             language = \"en\",\n",
    "                             divide_users = True,\n",
    "                             sender = sender\n",
    "                            )\n",
    "        \n",
    "        ## and we find word matches \n",
    "        occur = FindMatch(cl_data = cl_data, \n",
    "                          wordlist = wordlist, \n",
    "                          period = period,\n",
    "                         sender=sender)\n",
    "        return occur\n",
    "    #######\n",
    "        \n",
    "    ## if we want to investigate on single users (RQ1)\n",
    "    else:\n",
    "        \n",
    "        ## we clean our dataa\n",
    "        cl_data = clean_data(data = data,\n",
    "                             language = language,)\n",
    "        \n",
    "        ## and we find word matches\n",
    "        occur = FindMatch(cl_data = cl_data, \n",
    "                          wordlist = wordlist, \n",
    "                          period = period)\n",
    "        return occur\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5580d4f",
   "metadata": {},
   "source": [
    "<h2 id=\"merge\"> RQ1: Merging the data </h2>\n",
    "<h3> Importing the data </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cbdbf7",
   "metadata": {},
   "source": [
    "<p> We first consider the case in which we merge all the <tt>messages.json</tt> (<tt> divide_users = False </tt>)\n",
    "<p> Let's import our <tt>messages.json</tt> and merge them together in order to  obtain the argument <tt>data</tt>:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3290d489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, ['participants', 'conversation'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os ## way of using operating system dependent functionality.\n",
    "\n",
    "materials = os.getcwd()+\"/materials\" ## the directory 'materials' in our current working directory\n",
    "subdir = next(os.walk(materials))[1] ## every subdirectory in a given directory\n",
    "\n",
    "messages = []\n",
    "for d in subdir:\n",
    "        s_directory = materials+\"/\"+d\n",
    "        try: open(s_directory+\"/messages.json\",'r')\n",
    "        except FileNotFoundError: pass \n",
    "        else:\n",
    "            with open(s_directory+\"/messages.json\",'r') as jfile:\n",
    "                messages.extend(json.load(jfile)) ##append every json in result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "find_keys(messages) ##let's check if the structure is the same for all the json files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8716a6e2",
   "metadata": {},
   "source": [
    "<h3> Defining the wordlist and the period </h3>\n",
    "<p> Let's then define our argument <tt>wordlist</tt>, which is going to be <tt>articles</tt>, and <tt>period1</tt>, <tt>period2</tt> (we will launch the function twice), which are going to be our argument <tt>period</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d1c7699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime ## python datetime library\n",
    "import pytz ## accurate and cross platform timezone calculations\n",
    "\n",
    "##wordlist\n",
    "articles = [\" a \",\" the \",\" an \"]    \n",
    "\n",
    "##period\n",
    "period1 = [datetime.datetime(year=2020, month=10, day=7, tzinfo=pytz.UTC), \n",
    "           datetime.datetime(year=2020, month=10, day=14, tzinfo=pytz.UTC)]\n",
    "\n",
    "##period\n",
    "period2 = [datetime.datetime(year=2020, month=10, day=16, tzinfo=pytz.UTC),\n",
    "           datetime.datetime(year=2020, month=10, day=23, tzinfo=pytz.UTC)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6924d7b",
   "metadata": {},
   "source": [
    "<h3> Running the script </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0715a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** OVERVIEW PERIOD 1 *****\n",
      "{\n",
      "   \"sender\": \"multiple senders\",\n",
      "   \"filtered_data\": 22,\n",
      "   \"matches\": 7,\n",
      "   \"ratio\": 0.3181818181818182\n",
      "}\n",
      "***** SOME CORPUS PERIOD 1 *****\n",
      "[\n",
      "   {\n",
      "      \"date\": \"2020-10-13T09:55:12.968106+00:00\",\n",
      "      \"text\": \"say yes to the dress\",\n",
      "      \"sender\": \"adfb70ad-69bc-5967-aeed-0289e7d859d7\"\n",
      "   },\n",
      "   {\n",
      "      \"date\": \"2020-10-12T20:15:43.753243+00:00\",\n",
      "      \"text\": \"you can look who follows me an add them, they are all participants\",\n",
      "      \"sender\": \"c4ee6575-6f58-5a54-9c6d-1093133ce5cc\"\n",
      "   },\n",
      "   {\n",
      "      \"date\": \"2020-10-12T20:15:59.416476+00:00\",\n",
      "      \"text\": \"you can look who follows me an add them, they are all participants\",\n",
      "      \"sender\": \"c4ee6575-6f58-5a54-9c6d-1093133ce5cc\"\n",
      "   }\n",
      "]\n",
      "-------------\n",
      "***** OVERVIEW PERIOD 2 *****\n",
      "{\n",
      "   \"sender\": \"multiple senders\",\n",
      "   \"filtered_data\": 135,\n",
      "   \"matches\": 24,\n",
      "   \"ratio\": 0.17777777777777778\n",
      "}\n",
      "***** SOME CORPUS PERIOD 2 *****\n",
      "[\n",
      "   {\n",
      "      \"date\": \"2020-10-22T15:49:43.838170+00:00\",\n",
      "      \"text\": \"or maybe if we all add send you our numbers we can have a video call\",\n",
      "      \"sender\": \"a1f3ccd0-e880-51a3-a69b-fe427de503ee\"\n",
      "   },\n",
      "   {\n",
      "      \"date\": \"2020-10-22T15:49:00.030639+00:00\",\n",
      "      \"text\": \"thanks for your message tom! i\\u2019ll give you a call this weekend and we can catch-up\",\n",
      "      \"sender\": \"a1f3ccd0-e880-51a3-a69b-fe427de503ee\"\n",
      "   },\n",
      "   {\n",
      "      \"date\": \"2020-10-22T14:03:13.377673+00:00\",\n",
      "      \"text\": \"hey! give me a call at **num**! or mail me at **email**\",\n",
      "      \"sender\": \"d1c4de05-ed32-511d-92a1-aef9a8415e2a\"\n",
      "   }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "result_1_1 = start_match(\n",
    "    data = messages,\n",
    "    wordlist = articles,\n",
    "    period = period1)\n",
    "\n",
    "\n",
    "result_1_2 = start_match(\n",
    "    data = messages,\n",
    "    wordlist = articles,\n",
    "    period = period2)\n",
    "\n",
    "\n",
    "print(\"***** OVERVIEW PERIOD 1 *****\")\n",
    "print(json.dumps(result_1_1.overview,indent=3))\n",
    "print(\"***** SOME CORPUS PERIOD 1 *****\")\n",
    "print(json.dumps(result_1_1.matches_all[:3],indent=3))\n",
    "print(\"-------------\")\n",
    "print(\"***** OVERVIEW PERIOD 2 *****\")\n",
    "print(json.dumps(result_1_2.overview,indent=3))\n",
    "print(\"***** SOME CORPUS PERIOD 2 *****\")\n",
    "print(json.dumps(result_1_2.matches_all[:3],indent=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe55ca00",
   "metadata": {},
   "source": [
    "<h3> Results and Discussion </h3><span><p>calculated in R </p></span>\n",
    "<p> We can see that the ratio during period1 ( <i>r</i>=.32 ) is higher than the ratio in period2  ( <i>r</i>=.17 ). However, we see that the <tt>filtered_data</tt> from both period contains very small data, which is in contrast with the general computational analysis, which usually works with big databases. In other words, to investigate on significant changes in the two periods, we need to analyse a bigger sample of <tt class=\"str\">'messages.json'</tt>.<p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba738e",
   "metadata": {},
   "source": [
    "<h2 id=\"merge\"> RQ2: Analysing every single user </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425764b",
   "metadata": {},
   "source": [
    "<p> Let's consider RQ2 (<tt> divide_users = True </tt>). As we don't need to merge the json, let's launch <tt class=\"fun\"> start_match()</tt> while we iterate trough files. </p>\n",
    "<p> We already have our <tt>period</tt> and <tt>wordlist</tt> arguments. We only need to change argument <tt>data</tt> and argument <tt>directory</tt> while looping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2a34d53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** OVERVIEW PERIOD 1 ********\n",
      "====================================  ===============  =========  =======\n",
      "sender                                  filtered_data    matches    ratio\n",
      "====================================  ===============  =========  =======\n",
      "b449fd02-660b-57b3-8cb4-9ab5883dbbd2                0          0\n",
      "90a7952f-8cb3-5e45-b276-6472f3b02cbf                0          0\n",
      "774a1aa2-85a8-5be8-bb89-d590e655c02b                1          0        0\n",
      "077e7fde-9e3f-57e9-baeb-b418b9a0ee06                0          0\n",
      "c4ee6575-6f58-5a54-9c6d-1093133ce5cc                2          2        1\n",
      "dab501a0-9984-58f2-bf5a-6aed28efebcc                1          1        1\n",
      "af5eedbb-6e9d-57a2-811d-4f369f34469c                1          0        0\n",
      "c97ab426-aab5-5850-b5ce-8c749f9cdaef                1          0        0\n",
      "1b49d9fb-5747-55f3-b9cd-0b53d3f8b35c                0          0\n",
      "c4ee6575-6f58-5a54-9c6d-1093133ce5cc                0          0\n",
      "====================================  ===============  =========  =======\n",
      "\n",
      "******** OVERVIEW PERIOD 2 ********\n",
      "====================================  ===============  =========  ========\n",
      "sender                                  filtered_data    matches     ratio\n",
      "====================================  ===============  =========  ========\n",
      "b449fd02-660b-57b3-8cb4-9ab5883dbbd2                9          2  0.222222\n",
      "90a7952f-8cb3-5e45-b276-6472f3b02cbf                8          2  0.25\n",
      "774a1aa2-85a8-5be8-bb89-d590e655c02b                0          0\n",
      "077e7fde-9e3f-57e9-baeb-b418b9a0ee06               18          3  0.166667\n",
      "c4ee6575-6f58-5a54-9c6d-1093133ce5cc                1          0  0\n",
      "dab501a0-9984-58f2-bf5a-6aed28efebcc               15          0  0\n",
      "af5eedbb-6e9d-57a2-811d-4f369f34469c                5          0  0\n",
      "c97ab426-aab5-5850-b5ce-8c749f9cdaef                2          1  0.5\n",
      "1b49d9fb-5747-55f3-b9cd-0b53d3f8b35c                3          1  0.333333\n",
      "c4ee6575-6f58-5a54-9c6d-1093133ce5cc                0          0\n",
      "====================================  ===============  =========  ========\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import tabulate\n",
    "\n",
    "\n",
    "result_2_1 = []\n",
    "\n",
    "for d in subdir:\n",
    "    s_directory = materials+\"/\"+d  # directory\n",
    "    try: open(s_directory+\"/messages.json\",'r')\n",
    "    except FileNotFoundError: pass\n",
    "    else:\n",
    "        with open(s_directory+\"/messages.json\",'r') as jfile:\n",
    "            s_messages = json.load(jfile)  #data\n",
    "            \n",
    "            s_result_1 = start_match(\n",
    "                    data = s_messages,\n",
    "                    wordlist = articles,\n",
    "                    period = period1,\n",
    "                    directory = s_directory,\n",
    "                    divide_users=True)\n",
    "            \n",
    "            result_2_1.append(s_result_1)\n",
    "\n",
    "result_2_2 = []\n",
    "\n",
    "for d in subdir:\n",
    "    s_directory = materials+\"/\"+d\n",
    "    try: open(s_directory+\"/messages.json\",'r')\n",
    "    except FileNotFoundError: pass\n",
    "    else:\n",
    "        with open(s_directory+\"/messages.json\",'r') as jfile:\n",
    "            s_messages = json.load(jfile)   \n",
    "            \n",
    "            s_result_2 = start_match(\n",
    "                    data = s_messages,\n",
    "                    wordlist = articles,\n",
    "                    period = period2,\n",
    "                    directory = s_directory,\n",
    "                    divide_users=True)\n",
    "            result_2_2.append(s_result_2)\n",
    "            \n",
    "            \n",
    "\n",
    "ratio1 = []        \n",
    "[ratio1.append(res1.overview) for res1 in result_2_1 ]\n",
    "\n",
    "ratio2 = []        \n",
    "[ratio2.append(res2.overview) for res2 in result_2_2 ]\n",
    "\n",
    "\n",
    "header1 = ratio1[0].keys()\n",
    "rows1 =  [x.values() for x in ratio1]\n",
    "header2 = ratio2[0].keys()\n",
    "rows2 =  [x.values() for x in ratio2]\n",
    "\n",
    "print(\"******** OVERVIEW PERIOD 1 ********\")\n",
    "print(tabulate.tabulate(rows1, header1,tablefmt='rst'))\n",
    "print(\"\")\n",
    "print(\"******** OVERVIEW PERIOD 2 ********\")\n",
    "print(tabulate.tabulate(rows2, header2,tablefmt='rst'))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1db70b",
   "metadata": {},
   "source": [
    "<h3> Results and Discussion </h3><span><p>calculated in R </p></span>\n",
    "<p> We perform a within-subject ANOVA to investigate differences ratios between the two periods. We find no significant differences, <i>F(1,12)</i>=1.1, <i>p</i>=.31 .</p>\n",
    "<p>\n",
    "For the reasons we highlighted during RQ1 discussion, we acknowledge that the sample size is too little to expect a significant difference between the two periods. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf002a7",
   "metadata": {},
   "source": [
    "<h2> Conclusion </h2>\n",
    "<p>We could not find significant evidence for the invented scenario through our answers to both RQ1 and RQ2. \n",
    "<br/>\n",
    "However, we can now apply the script to the study of every other wordlist and every other Instagram dataset. In other words, we can perform the same analysis on a sample of millions of users potentially. By developing the script, we wanted to make sure that it was scalable and functional when faced with other, more realistic scenarios. These include sentiment analysis to prevent the Werther effect's suicides (Yip & Pinkney, 2022), detecting political pulse (Salgado & Sanz, 2022), and all major social movements and media events that had and will engage users in the future years. In addition, if we can ask the users to donate us their Instagram private data, we will also be able to categorize every user with additional parameters such as gender, age, and location. This would lead to performing studies in a more targeted way. \n",
    "<br/>\n",
    "Future works may focus on finding the best condition to develope a project such as the D3I. In this way, we will be able to finally analyse a large sample of personal data such as the one used in this script and extract all the information we cannot reach from users' public data. This will allow us to finally get closer to the general understanding of human behavior in digital environments.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4427be",
   "metadata": {},
   "source": [
    "<h2> References </h2>\n",
    "<ul>\n",
    "<li>\n",
    "Choi, S., Williams, D., & Kim, H. (2020). A snap of your true self: How self-presentation and temporal affordance influence self-concept on social media. New Media & Society. <a>https://doi.org/10.1177/1461444820977199</a> </li>\n",
    "<br/>\n",
    "<li>Folgado, M.G., Sanz, V. (2022)Exploring the political pulse of a country using data science tools. J Comput Soc Sc. <a>https://doi.org/10.1007/s42001-021-00157-1</a></li>\n",
    "<br/>    \n",
    "    <li>Gleason, B. (2013). #Occupy Wall Street: Exploring Informal Learning About a Social Movement on Twitter. American Behavioral Scientist, 57(7), 966–982. <a>https://doi.org/10.1177/0002764213479372</a>\n",
    "    </li>  \n",
    "    <br/>\n",
    "<li>    \n",
    "Hong, S., & Kim, S. H. (2016). Political polarization on Twitter: Implications for the use of social media in digital governments. Government Information Quarterly, 33(4), 777–782. <a>https://doi.org/10.1016/j.giq.2016.04.007.E13 </a>\n",
    "    </li>\n",
    "    \n",
    "<br/>\n",
    "\n",
    "<li>\n",
    "Jelani Ince, Fabio Rojas & Clayton A. Davis (2017) The social media response to Black Lives Matter: how Twitter users interact with Black Lives Matter through hashtag use, Ethnic and Racial Studies, 40:11, 1814-1830, DOI: <a>https://doi.org/10.1080/01419870.2017.1334931</a>\n",
    "    </li>\n",
    "    <br/>\n",
    "<li>\n",
    "Lomanowska, A. M., & Guitton, M. J. (2016). Online intimacy and well-being in the digital age. Internet Interventions, 4, 138–144. doi: <a>https://doi.org/10.1016/j.invent.2016.06.005 </a>\n",
    "    </li>\n",
    "    <br/> \n",
    "<li>\n",
    "Yip, P.S.F., Pinkney, E. (2022) Social media and suicide in social movements: a case study in Hong Kong. J Comput Soc Sc. <a>https://doi.org/10.1007/s42001-022-00159-7</a>\n",
    "    </li>\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21c1f45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1,h2{\n",
       "    color:#D37171\n",
       "}\n",
       "h3,p,li {\n",
       "    \n",
       "    color: #717070;\n",
       "    line-height:150%\n",
       "}\n",
       "\n",
       ".cont {\n",
       "  width:100%;\n",
       "  text-align:center;\n",
       "  display: inline-grid;\n",
       "  justify-content:center;\n",
       "flex-direction: column;\n",
       "}\n",
       "\n",
       ".t_cont {\n",
       "    width:400px\n",
       "}\n",
       "\n",
       ".fun {\n",
       "    color:blue\n",
       "}\n",
       ".str {\n",
       "    color: #AB312A\n",
       "}\n",
       "\n",
       ".fun,.str{\n",
       "    font-weight:400\n",
       "}\n",
       ".appendix{\n",
       "    font-size:12px;\n",
       "}\n",
       "\n",
       ".img {\n",
       "    width:50%;\n",
       "    margin-top:50px;\n",
       "    margin-bottom:50px;\n",
       "    \n",
       "}\n",
       "\n",
       "tt{\n",
       "    font-weight:800\n",
       "}\n",
       "\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "h1,h2{\n",
    "    color:#D37171\n",
    "}\n",
    "h3,p,li {\n",
    "    \n",
    "    color: #717070;\n",
    "    line-height:150%\n",
    "}\n",
    "\n",
    ".cont {\n",
    "  width:100%;\n",
    "  text-align:center;\n",
    "  display: inline-grid;\n",
    "  justify-content:center;\n",
    "flex-direction: column;\n",
    "}\n",
    "\n",
    ".t_cont {\n",
    "    width:400px\n",
    "}\n",
    "\n",
    ".fun {\n",
    "    color:blue\n",
    "}\n",
    ".str {\n",
    "    color: #AB312A\n",
    "}\n",
    "\n",
    ".fun,.str{\n",
    "    font-weight:400\n",
    "}\n",
    ".appendix{\n",
    "    font-size:12px;\n",
    "}\n",
    "\n",
    ".img {\n",
    "    width:50%;\n",
    "    margin-top:50px;\n",
    "    margin-bottom:50px;\n",
    "    \n",
    "}\n",
    "\n",
    "tt{\n",
    "    font-weight:800\n",
    "}\n",
    "\n",
    "\n",
    "</style>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
